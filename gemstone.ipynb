{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gemstone.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/yesxiaoyu/git-demo/blob/master/gemstone.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "YxgjDGsRFyPM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !git config --global user.name \"hongyu\"\n",
        "# !git config --global user.email \"1757732597@qq.com\"\n",
        "# !git init\n",
        "# !git add baseline.csv\n",
        "# !git commit -m \"submission\"\n",
        "# !git remote add origin https://yesxiaoyu:ace1587578963@github.com/yesxiaoyu/git-demo.git\n",
        "# !git push origin master\n",
        "# !git remote rm origin\n",
        "# !git remote add origin https://github.com/yesxiaoyu/git-demo\n",
        "# !git pull --rebase origin master\n",
        "# !git push -u origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UAt77h4JJQhy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-h_SqIpSGbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive  -o nonempty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zg-NmGYpTH-m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxbS6BOZp6wk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5DJ1L01Ip-cM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VAAKzC2Zp_gz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd Data Mining"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hyw4ftaTqGu7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3lU5sbBsqXri",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xzvf rarlinux-x64-5.6.0.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQlgXV2Bs4AD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd rar/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Ws8p8u9uEwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uVEIusEXuGC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZgdpdegZuORH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qhCrVj7GuPmp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unrar e gemstone_data.rar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MC9r6AjcuVdn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D66Rc7x-tfyY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AMPhnmRvvZPi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install jieba"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1kXCoBmLRQz5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lAwcfDw0Rg9a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p8LorTlHRoff",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VnlpfsGPRdX9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# coding:utf-8\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras.backend.tensorflow_backend as KTF\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True  # 不全部占满显存, 按需分配\n",
        "session = tf.Session(config=config)\n",
        "\n",
        "# 设置session\n",
        "KTF.set_session(session)\n",
        "\n",
        "import jieba\n",
        "from gensim.models import word2vec, Word2Vec\n",
        "import pandas as pd\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1 读取数据\n",
        "train = pd.read_csv('data_train.csv', encoding='gbk', sep='\\t', header=None)\n",
        "test = pd.read_csv('data_test.csv', encoding='gbk', sep='\\t', header=None)\n",
        "sub = pd.DataFrame()\n",
        "sub['index'] = list(test[0].values)\n",
        "\n",
        "# 合并数据\n",
        "train_and_test = pd.concat([train, test])\n",
        "\n",
        "# 2 分词\n",
        "with open('base_test_datas_1.txt', 'w', encoding='utf8') as f:\n",
        "    for i in train_and_test.values:\n",
        "        f.write(str(' '.join([x for x in jieba.cut(str(i[2]))])) + '\\n')\n",
        "f.close()\n",
        "\n",
        "# 3 训练词向量\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "sentences = word2vec.Text8Corpus('base_test_datas_1.txt')\n",
        "model = Word2Vec(sentences, size=300, window=10, min_count=1, sg=1, iter=10)\n",
        "model.save('w2v_300_sg_x.model')\n",
        "\n",
        "X_train = train[[2]]\n",
        "X_test = test[[2]]\n",
        "texts = pd.concat([X_train, X_test], axis=0)\n",
        "texts[2] = texts[2].apply(lambda x: ' '.join(x for x in jieba.cut(str(x))))\n",
        "texts = texts[2].values\n",
        "\n",
        "# 3 深度模型\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, Flatten, Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Convolution1D\n",
        "from keras.models import Model\n",
        "from keras.layers import concatenate\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000  # 每篇文章选取1000个词\n",
        "# 设置词典的数目\n",
        "MAX_NB_WORDS = 40000  # 将字典设置为含有1万个词\n",
        "# 维度\n",
        "EMBEDDING_DIM = 300  # 词向量维度，300维\n",
        "\n",
        "# 选取词频最高的一部分词\n",
        "\n",
        "embedings_index = {}\n",
        "null_word = np.zeros(EMBEDDING_DIM)\n",
        "word_vectors = model.wv\n",
        "null_word_count = 0\n",
        "# 构造自己训练的词向量模型\n",
        "for word, vocab_obj in model.wv.vocab.items():\n",
        "    # if int(vocab_obj.index) < MAX_NB_WORDS:\n",
        "    try:\n",
        "        embedings_index[word] = model[word]\n",
        "    except:\n",
        "        embedings_index[word] = null_word\n",
        "        null_word_count = null_word_count + 1\n",
        "'''\n",
        "embeddings_index_baidubaike = {}\n",
        "with open('../tmp/sgns.target.word-ngram.1-2.dynwin5.thr10.neg5.dim300.iter5',errors='ignore') as f:\n",
        "    for line in f:\n",
        "        #print(line)\n",
        "        try:\n",
        "            #line = line.decode('utf-8', 'ignore')\n",
        "            #line = line.decode('utf-8')\n",
        "            values = line.rstrip().rsplit(' ')\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index_baidubaike[word] = coefs\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print('baidubaike',len(embeddings_index_baidubaike))\n",
        "'''\n",
        "\n",
        "# print(embeddings_index_baidubaike)\n",
        "# print(embeddings_index_baidubaike)\n",
        "# print(embeddings_index_baidubaike)\n",
        "del model, word_vectors\n",
        "print('word vectors ', len(embedings_index))\n",
        "print('Found %s null word.' % null_word_count)\n",
        "\n",
        "# 文本准备\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)  # 传入我们词向量的字典\n",
        "tokenizer.fit_on_texts(texts)  # 传入我们的训练数据，得到训练数据中出现的词的字典\n",
        "sequences = tokenizer.texts_to_sequences(texts)  # 根据训练数据中出现的词的字典，将训练数据转换为sequences\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  # 限制每篇文章的长度\n",
        "# labels = train[[3]].values.tolist()\n",
        "# labels = to_categorical(np.asarray(labels)) # label one hot表示\n",
        "print('Shape of data tensor:', data.shape)\n",
        "# print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# 划分训练集和测试集合\n",
        "X = data[:train.shape[0]]\n",
        "y = train[[3]].values\n",
        "X_sub = data[train.shape[0]:]\n",
        "\n",
        "print('Shape of train', X.shape)\n",
        "print('Shape of train', y.shape)\n",
        "print('Shape of ttest', X_sub.shape)\n",
        "\n",
        "# 单词 向量\n",
        "# 单词 index\n",
        "# index 向量\n",
        "\n",
        "'''\n",
        "单词1 词向量\n",
        "单词2 词向量\n",
        "'''\n",
        "num_words = min(MAX_NB_WORDS, len(word_index))  # 对比词向量字典中包含词的个数与文本数据所有词的个数，取小\n",
        "print(num_words)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    # print(i,word)\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embedings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # 文本数据中的词在词向量字典中没有，向量为取0；如果有则取词向量中该词的向量\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    # else:\n",
        "    # using self training word vectory to filling\n",
        "    # embedding_matrix[i] = embedings_index.get(word)\n",
        "\n",
        "# 将预训练好的词向量加载如embedding layer\n",
        "# 我们设置 trainable = False，代表词向量不作为参数进行更新\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    # 训练  1D 卷积神经网络 使用 Maxpooling1D\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    x1 = Convolution1D(filters=32, kernel_size=2, activation='relu')(embedded_sequences)\n",
        "    x1 = MaxPooling1D(pool_size=27)(x1)\n",
        "    x1 = Flatten()(x1)\n",
        "\n",
        "    x2 = Convolution1D(filters=32, kernel_size=3, activation='relu')(embedded_sequences)\n",
        "    x2 = MaxPooling1D(pool_size=26)(x2)\n",
        "    x2 = Flatten()(x2)\n",
        "\n",
        "    x3 = Convolution1D(filters=32, kernel_size=4, activation='relu')(embedded_sequences)\n",
        "    x3 = MaxPooling1D(pool_size=25)(x3)\n",
        "    x3 = Flatten()(x3)\n",
        "\n",
        "    x4 = Convolution1D(filters=32, kernel_size=6, activation='relu')(embedded_sequences)\n",
        "    x4 = MaxPooling1D(pool_size=24)(x4)\n",
        "    x4 = Flatten()(x4)\n",
        "\n",
        "    x = concatenate([x1, x2, x3, x4], axis=-1)\n",
        "    # x = Dropout(0.25)(x)\n",
        "    # x = Dense(128, activation='relu')(x)\n",
        "    preds = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "N = 5\n",
        "skf = StratifiedKFold(n_splits=N, random_state=42, shuffle=True)\n",
        "cv_pred = []\n",
        "xx_score = []\n",
        "f1 = []\n",
        "for k, (train_in, test_in) in enumerate(skf.split(X, y)):\n",
        "    print(k)\n",
        "    X_train, X_test, y_train, y_test = X[train_in], X[test_in], y[train_in], y[test_in]\n",
        "    # print((y_test))\n",
        "    # print((y_train))\n",
        "    # exit()\n",
        "    y_train = to_categorical(np.array(y_train))\n",
        "    y_test = to_categorical(np.array(y_test))\n",
        "    # print((y_test))\n",
        "    # print((y_train))\n",
        "    # exit()\n",
        "    print('X_train', X_train.shape)\n",
        "    print('y_train', y_train.shape)\n",
        "\n",
        "    print('X_test', X_test.shape)\n",
        "    print('y_test', y_test.shape)\n",
        "\n",
        "    model = get_model()\n",
        "\n",
        "    checkpointer = ModelCheckpoint(filepath=\"./checkpoint.hdf5\", monitor='val_acc', verbose=1, save_best_only=True,\n",
        "                                   mode='auto')\n",
        "    early = EarlyStopping(monitor='val_acc', patience=3, verbose=0, mode='auto')\n",
        "    # 如果希望短一些时间可以，epochs调小\n",
        "    model.fit(X_train, y_train,\n",
        "              batch_size=512,\n",
        "              epochs=50,\n",
        "              validation_data=(X_test, y_test),\n",
        "              callbacks=[checkpointer, early])\n",
        "\n",
        "    model.load_weights(\"./checkpoint.hdf5\")\n",
        "\n",
        "    test_y = model.predict(X_test, batch_size=128)\n",
        "    print(test_y)\n",
        "\n",
        "    xx_res = []\n",
        "    xs_res = []\n",
        "    for i in test_y:\n",
        "        xx_res.append(np.argmax(i))\n",
        "\n",
        "    for i in y_test:\n",
        "        xs_res.append(np.argmax(i))\n",
        "\n",
        "    # print(xx_res)\n",
        "    # print(xs_res)\n",
        "    # exit()\n",
        "\n",
        "    f1_s = f1_score(xs_res, xx_res, average='weighted')\n",
        "    f1.append(f1_s)\n",
        "\n",
        "    y_sub = model.predict(X_sub, batch_size=128)\n",
        "    # sub['submit'] = list(y_sub[:,0])\n",
        "    # sub[['index','submit']].to_csv('./testcnn_one_flod.csv',index=False,header=None)\n",
        "    # del sub['submit']\n",
        "\n",
        "    tmp_res = []\n",
        "    for i in y_sub:\n",
        "        tmp_res.append(np.argmax(i))\n",
        "\n",
        "    if k == 0:\n",
        "        cv_pred = np.array(tmp_res).reshape(-1, 1)\n",
        "    else:\n",
        "        cv_pred = np.hstack((cv_pred, np.array(tmp_res).reshape(-1, 1)))\n",
        "\n",
        "submit = []\n",
        "for line in cv_pred:\n",
        "    submit.append(np.argmax(np.bincount(line)))\n",
        "\n",
        "print(np.mean(f1))\n",
        "\n",
        "sub['submit'] = list(submit)\n",
        "sub[['index', 'submit']].to_csv('testcnn.csv', index=False, header=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E3N66E5RRcd2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGFgnCJJsuhM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8AWl2YBHUBpW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-rtlOyUUBV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PJaqgonuUBNR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dO3UjspyUBEW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEg8Po33UA3Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}